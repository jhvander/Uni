---
title: "Mandatory Assignment 3"
author: "Emil H. Erbas, Jeppe Vanderhaegen and Jacob G. Vestergaard"
date: "2024/05/10"
execute: 
  echo: false
  warning: false
format:
  pdf: 
    number-sections: false
    colorlinks: true
    geometry:
      - top=20mm
      - left=20mm
      - bottom=20mm
      - right=20mm
    cite-method: natbib
    fontsize: 12pt
---

## Exercise 1
We import the monthly CRSP dataset, filtering out stocks, that lack a consistent monthly data history of excess returns. We group data by each \texttt{permno}, and check for the presence of all required monthly dates within the period, spanning from 1962 to 2020. After the filtering, 119 stocks are present.
```{r}
# Loading libraries
library(tidyverse)
library(kableExtra)
library(RSQLite)
library(ggplot2)
library(dplyr)
library(lubridate)
library(broom)
library(scales)
library(purrr)
library(nloptr)
library(quadprog)
library(knitr)
library(lmtest)
library(sandwich)
```

```{r}
# Setting up Tidy Finance
tidy_finance <- dbConnect(SQLite(), 
                          "/Users/jacob/Desktop/Repo/data/tidy_finance_r.sqlite", 
                          extended_types = TRUE)

# Loading in CRSP data    
crsp_monthly <- tbl(tidy_finance, "crsp_monthly") %>%
 select(permno, month, ret_excess) %>%
  collect()

# Define the filtering range
start_date <- as.Date("1962-01-01")
end_date <- as.Date("2020-12-31")

# Generate the complete sequence of months from 1962 to 2020
all_months <- seq(from = start_date, to = end_date, by = "month")

# Filter by the date range and for permno with complete data
crsp_monthly_filtered <- crsp_monthly %>%
  # Keep only records within the desired date range
  filter(month >= start_date & month <= end_date) %>%
  group_by(permno) %>%
  # Check if all months from 1962 to 2020 are present
  filter(all(all_months %in% month)) %>%
  ungroup()

# Count the number of unique permnos using base R functions
num_unique_permnos <- length(unique(crsp_monthly_filtered$permno))
```

## Exercise 2
We consider the portfolio choice problem with transaction-costs:
$$\omega^{\star}_{t+1}=\arg \max_{\omega\in\mathbb{R}^N,\iota'\omega=1}\omega'\hat{\mu}-\lambda(\omega-\omega_{t^+})'\Sigma(\omega-\omega_{t^+})-\frac{\gamma}{2}\omega'\hat{\Sigma}\omega$$
To derive a closed-form solution for the mean-variance efficient portfolio $\omega^{\star}_{t+1}$, we use Lemma 2 in the Online Appendix from Hautsch et al. (2019). The same portfolio choice problem with the following transaction-costs:
$$\nu_t(\omega,\omega_{t^+},\beta)=\frac{\beta}{2}(\omega-\omega_{t^+})'\Sigma(\omega-\omega_{t^+})$$
Can be reformulated to:
$$\arg \min_{\omega\in\mathbb{R}^N,\iota'\omega=1} \frac{\gamma}{2}\omega'\tilde{\Sigma}\omega-\omega'\tilde{\mu}$$
With:
$$\tilde{\Sigma}:=\left(1+\frac{\beta}{\gamma}\right)\Sigma$$
$$\tilde{\mu}:=\mu+\beta\Sigma\omega_{t^+}$$
We can easily substitute in $\lambda=\frac{\beta}{2}\rightarrow\beta=2\lambda$:
$$\tilde{\Sigma}:=\left(1+\frac{2\lambda}{\gamma}\right)\Sigma$$
$$\tilde{\mu}:=\mu+2\lambda\Sigma\omega_{t^+}$$
With the known solution for the mean-variance efficient portfolio:
$$\omega(\tilde{\mu},\tilde{\Sigma},\gamma)=\frac{1}{\gamma}\left(\tilde{\Sigma}^{-1}-\frac{1}{\iota'\tilde{\Sigma}^{-1}\iota}\tilde{\Sigma}^{-1}\iota\iota'\tilde{\Sigma}^{-1}\right)\tilde{\mu}+\frac{1}{\iota'\tilde{\Sigma}^{-1}\iota}\tilde{\Sigma}^{-1}\iota$$
Transaction costs are modeled to be proportional to volatility. In financial markets, investors primarily encounter two types of transaction costs. First, there are typical costs like brokerage fees, and second, market costs such as the bid-ask spread. While brokerage fees are usually fixed, Frank et al. (2009) found that volatility positively correlates with the spread. This finding suggests that the assumption of transaction costs being proportional to volatility is reasonable. However, one could argue that also including a component representing fixed costs would better reflect the real-world investor experience.

We define the function $\texttt{compute\_efficient\_weight}$, which receives the inputs $\hat{\mu},\hat{\Sigma},\gamma,\lambda$. This function calculates $\omega^{\star}_{t+1}$, based on the current weights $\omega_{t^+}$.

```{r}
# Transform data for Sigma estimation
data_wide <- crsp_monthly_filtered |>
  pivot_wider(names_from = permno, values_from = ret_excess) |>
  select(-month)

# Calculating mu
mu <- colMeans(data_wide, na.rm = TRUE)  #Mean Returns 
```

```{r}
compute_ledoit_wolf <- function(x) {
  # Computes Ledoit-Wolf shrinkage covariance estimator
  # This function generates the Ledoit-Wolf covariance estimator  as proposed in Ledoit, Wolf 2004 (Honey, I shrunk the sample covariance matrix.)
  # X is a (t x n) matrix of returns
  t <- nrow(x)
  n <- ncol(x)
  x <- apply(x, 2, function(x) if (is.numeric(x)) # demean x
    x - mean(x) else x)
  sample <- (1/t) * (t(x) %*% x)
  var <- diag(sample)
  sqrtvar <- sqrt(var)
  rBar <- (sum(sum(sample/(sqrtvar %*% t(sqrtvar)))) - n)/(n * (n - 1))
  prior <- rBar * sqrtvar %*% t(sqrtvar)
  diag(prior) <- var
  y <- x^2
  phiMat <- t(y) %*% y/t - 2 * (t(x) %*% x) * sample/t + sample^2
  phi <- sum(phiMat)

  repmat = function(X, m, n) {
    X <- as.matrix(X)
    mx = dim(X)[1]
    nx = dim(X)[2]
    matrix(t(matrix(X, mx, nx * n)), mx * m, nx * n, byrow = T)
  }

  term1 <- (t(x^3) %*% x)/t
  help <- t(x) %*% x/t
  helpDiag <- diag(help)
  term2 <- repmat(helpDiag, 1, n) * sample
  term3 <- help * repmat(var, 1, n)
  term4 <- repmat(var, 1, n) * sample
  thetaMat <- term1 - term2 - term3 + term4
  diag(thetaMat) <- 0
  rho <- sum(diag(phiMat)) + rBar * sum(sum(((1/sqrtvar) %*% t(sqrtvar)) * thetaMat))

  gamma <- sum(diag(t(sample - prior) %*% (sample - prior)))
  kappa <- (phi - rho)/gamma
  shrinkage <- max(0, min(1, kappa/t))
  if (is.nan(shrinkage))
    shrinkage <- 1
  sigma <- shrinkage * prior + (1 - shrinkage) * sample
  return(sigma)
}

Sigma <- compute_ledoit_wolf(data_wide)
```

```{r}
# Function to calculate efficient weights
compute_efficient_weight <- function(Sigma,
                                     mu,
                                     gamma,
                                     lambda, # transaction costs
                                     w_prev = rep(
                                       1 / ncol(Sigma),
                                       ncol(Sigma)
                                     )) {
  iota <- rep(1, ncol(Sigma))
  Sigma_processed <- Sigma * (1 + 2*lambda/gamma)
  mu_processed <- mu + lambda* 2 * (Sigma %*% w_prev)

  Sigma_inverse <- solve(Sigma_processed)

  w_mvp <- Sigma_inverse %*% iota
  w_mvp <- as.vector(w_mvp / sum(w_mvp))
  w_opt <- w_mvp + 1 / gamma *
    (Sigma_inverse - w_mvp %*% t(iota) %*% Sigma_inverse) %*%
      mu_processed
  
  return(as.vector(w_opt))
}
```

## Exercise 3
Our goal is to implement a comprehensive portfolio backtesting strategy, incorporating transaction costs proportional to risk. We set $\lambda = 0.02$ and $\gamma = 4$. For parameter estimation of the variance-covariance matrix $\hat{\Sigma}$, we use the Ledoit-Wolf shrinkage estimation method. This method is relatively straightforward, making it suitable for datasets with many assets but limited data points, as shrinkage helps mitigate overfitting. With 119 assets and only 708 data points per asset, the Ledoit-Wolf approach aligns well with our requirements. However, a limitation of this method is that it doesn't differentiate between asset correlations, assuming instead that they are relatively similar. The results can be observed in Table 1:

```{r}
window_length <- 120
periods <- nrow(data_wide) - window_length

lambda <- 200
gamma <- 4

performance_values <- matrix(NA,
  nrow = periods,
  ncol = 3
)
colnames(performance_values) <- c("raw_return", "turnover", "net_return")

performance_values <- list(
  "The transaction-cost adjusted portfolio" = performance_values,
  "The naive portfolio" = performance_values,
  "The portfolio without short-selling" = performance_values
)

w_prev_1 <- w_prev_2 <- w_prev_3 <- rep(
  1 / num_unique_permnos,
  num_unique_permnos
)
```

```{r}
adjust_weights <- function(w, next_return) {
  w_prev <- 1 + w * next_return
  as.numeric(w_prev / sum(as.vector(w_prev)))
}

evaluate_performance <- function(w, w_previous, next_return, lambda = 200) {
  raw_return <- as.matrix(next_return) %*% w
  turnover <- sum(abs(w - w_previous))
  net_return <- raw_return - lambda / 10000 * turnover
  c(raw_return, turnover, net_return)
}
```

```{r}
for (p in 1:periods) {
  returns_window <- data_wide[p:(p + window_length - 1), ]
  next_return <- data_wide[p + window_length, ] |> as.matrix()

  mu <- 0 * colMeans(returns_window)

  # Transaction-cost adjusted portfolio
  w_1 <- compute_efficient_weight(
    mu = mu,
    Sigma = Sigma,
    lambda = lambda,
    gamma = gamma,
    w_prev = rep(
                                       1 / ncol(Sigma),
                                       ncol(Sigma)
                                     )
  )

  performance_values[[1]][p, ] <- evaluate_performance(w_1,
    w_prev_1,
    next_return,
    lambda = lambda
  )

  w_prev_1 <- adjust_weights(w_1, next_return)

  # Naive portfolio
  w_2 <- rep(1 / num_unique_permnos, num_unique_permnos)

  performance_values[[2]][p, ] <- evaluate_performance(
    w_2,
    w_prev_2,
    next_return
  )

  w_prev_2 <- adjust_weights(w_2, next_return)
  
  # No short-selling
  w_3 <- solve.QP(Dmat = 2 * Sigma,
                  dvec = mu, 
                  Amat = cbind(1, diag(num_unique_permnos)), 
                  bvec = c(1, rep(0, num_unique_permnos)), 
                  meq = 1
  )$solution

  performance_values[[3]][p, ] <- evaluate_performance(w_3, 
                                                       w_prev_3, 
                                                       next_return)

  w_prev_3 <- adjust_weights(w_3, next_return)
}
```

```{r}
performance <- lapply(performance_values, as_tibble) %>% 
  bind_rows(.id = "Strategy")

performance <- performance %>%
  group_by(Strategy) %>%
  summarize(Mean = 12 * mean(100 * net_return),
            SD = sqrt(12) * sd(100 * net_return), 
            `Sharpe Ratio` = if_else(Mean > 0, Mean / SD, NA_real_),
            Turnover = 100 * mean(turnover)) %>%
  arrange(desc(Mean))

performance %>%
  knitr::kable(digits = 2, 
               caption = "Annualized strategy performance comparisons"
              ) %>%
  kable_styling(latex_options = "hold_position") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```
Our findings indicate that the naive portfolio performed the best, while the transaction-cost-adjusted portfolio followed closely. The no-short-selling portfolio performed the worst. This discrepancy is primarily because the no-short-selling strategy overlooks transaction costs, leading to significantly higher expenses due to increased turnover, which arises from frequent portfolio rebalancing. The strategy also limits the investor to a specific allocation, since shorting is not allowed.

In general, an out-of-sample experiment tests a strategy on an entirely new data set, ensuring that no data used for estimation is simultaneously used for evaluation. In contrast, our rolling window estimation traverses the whole data set, resulting in considerable overlap between estimation and evaluation data. Consequently, one could argue that backtesting is not a truly out-of-sample test.


